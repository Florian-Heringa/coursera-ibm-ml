{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dae8340",
   "metadata": {},
   "source": [
    "# Non-Negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "943d23cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2914acbb",
   "metadata": {},
   "source": [
    "Below, we read data from a text file that encode articles as embedded values. The data is represented as a sparse coordiante form `i j v` where `i` and `j` are indices and `v` is the value of the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b47d2f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1 1 1.0\\n', '1 7 2.0\\n', '1 11 1.0\\n', '1 14 1.0\\n', '1 15 2.0\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/bbc.mtx\", 'r') as f:\n",
    "    content = f.readlines()[2:]\n",
    "    \n",
    "content[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0cd829",
   "metadata": {},
   "source": [
    "First we process the data into numbers, representing values at specific coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "208d64bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,  86,   1],\n",
       "       [  1,  93,   1],\n",
       "       [  1,  99,   1],\n",
       "       [  1, 100,   1]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsemat = np.array(\n",
    "    [\n",
    "    tuple(\n",
    "        map(\n",
    "            int,\n",
    "            map( float, c.split() )\n",
    "            )\n",
    "        ) \n",
    "    for c in content]\n",
    ")\n",
    "\n",
    "sparsemat[25:29]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116fbb87",
   "metadata": {},
   "source": [
    "Then we can use the `coo_matrix` function from scipy to build a sparse matrix in coordinate form (also known as ijv, or triplet format). This means that the data is encoded as `A[i[k], j[k]] = data[k]`.\n",
    "\n",
    "For each datapoint, we pass the datapoint itself and the coordinates it should have in the matrix representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "056e8f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "coo = coo_matrix(\n",
    "    (sparsemat[:,2], (sparsemat[:, 0], sparsemat[:, 1]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07534796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 5, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], shape=(9636, 2226))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfde0bd",
   "metadata": {},
   "source": [
    "### NMF\n",
    "\n",
    "The NMF algorithm finds a factorization of the original matrix $X = WH$. With word embeddings this can be interpreted as a weight decomposition that gives the weight of a word on a topic.\n",
    "\n",
    "The input of the NMF algorithm is a matrix of shape (n_samples, n_features). The decomposition gives two matrices back of shape (n_samples, n_components) and (n_component, n_features).\n",
    "\n",
    "Using `fit_transform` below on our input data give the matrix that transforms the n_samples into n_components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23144e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9636, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NMF(n_components=5, init='random', random_state=818)\n",
    "doc_topic = model.fit_transform(coo)\n",
    "\n",
    "doc_topic.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea60618",
   "metadata": {},
   "source": [
    "For each of the input samples we can find which component has the greates weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82c7b25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, ..., 4, 4, 4], shape=(9636,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(doc_topic, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c62b63b",
   "metadata": {},
   "source": [
    "As we can see, the component mapping to features has the following shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36aafd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 2226)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.components_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0e6ef5",
   "metadata": {},
   "source": [
    "To actually map this back to the meaning of the words we need the original embedding information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32e7aebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ad',\n",
       " 'sale',\n",
       " 'boost',\n",
       " 'time',\n",
       " 'warner',\n",
       " 'profit',\n",
       " 'quarterli',\n",
       " 'media',\n",
       " 'giant',\n",
       " 'jump']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/bbc.terms\") as f:\n",
    "    content = f.readlines()\n",
    "words = [c.split()[0] for c in content]\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0eeceb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['bondi',\n",
       "  'stanlei',\n",
       "  'continent',\n",
       "  'mortgag',\n",
       "  'bare',\n",
       "  'least',\n",
       "  'extent',\n",
       "  '200',\n",
       "  'leav',\n",
       "  'frustrat',\n",
       "  'yuan',\n",
       "  'industri'],\n",
       " ['manipul',\n",
       "  'teenag',\n",
       "  'drawn',\n",
       "  'go',\n",
       "  'prosecutor',\n",
       "  'herbert',\n",
       "  'host',\n",
       "  'protest',\n",
       "  'hike',\n",
       "  'nation',\n",
       "  'calcul',\n",
       "  'power'],\n",
       " ['dimens',\n",
       "  'hous',\n",
       "  'march',\n",
       "  'wider',\n",
       "  'owner',\n",
       "  'intend',\n",
       "  'declin',\n",
       "  'forc',\n",
       "  'posit',\n",
       "  'founder',\n",
       "  'york',\n",
       "  'unavail'],\n",
       " ['rome',\n",
       "  'ft',\n",
       "  'regain',\n",
       "  'lawmak',\n",
       "  'outright',\n",
       "  'resum',\n",
       "  'childhood',\n",
       "  'greatest',\n",
       "  'citi',\n",
       "  'stagnat',\n",
       "  'crown',\n",
       "  'bodi'],\n",
       " ['build',\n",
       "  'empir',\n",
       "  'isol',\n",
       "  'Â£12',\n",
       "  'restructur',\n",
       "  'closer',\n",
       "  'plung',\n",
       "  'depreci',\n",
       "  'durham',\n",
       "  'race',\n",
       "  'juli',\n",
       "  'segreg']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words = []\n",
    "# Go through each of the components\n",
    "# Each row corresponds to a topic, where each value is the topic's word weight\n",
    "for c in model.components_:\n",
    "    # Sort each index by weight, keeping the index value \n",
    "    a = sorted(\n",
    "        [(v, i) for i, v in enumerate(c)], reverse=True\n",
    "    )\n",
    "    # Take the top 12 higest ranked words\n",
    "    a = a[:12]\n",
    "    # Grad the word that corresponds to the embedding index\n",
    "    topic_words.append(\n",
    "        [words[e[1]] for e in a]\n",
    "    )\n",
    "    \n",
    "topic_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ace0c",
   "metadata": {},
   "source": [
    "These word groupings *should* correspond to the original topics of the documents, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "691062c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['business', 'entertainment', 'politics', 'sport', 'tech'],\n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./data/bbc.docs\") as f:\n",
    "    doc_topics = np.unique(list(map(lambda l : l.split(\".\")[0], f.readlines())))\n",
    "doc_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a669f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm-ml-sD2suF2h-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
